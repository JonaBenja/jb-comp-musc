---
title: "Acoustic vs. Electronic Singer-Songwriters"
author: "Jona Benja Bosman"
output: 
  flexdashboard::flex_dashboard:
        storyboard: true
---

```{r setup, message=F}
library(flexdashboard)
library(tidyverse)
library(plotly)
library(Cairo)
library(spotifyr)
library(ggplot2)
library(plyr)
library(dplyr)
library(compmus)
library(knitr)
source('spotify.R')
```

Acoustic vs. Electronic Singer-Songwriters
-----------------------------------------------------------------------
### Introduction {data-commentary-width=500}

**Introduction**

This assignment will be about the differences and similarities between electronic and acoustic singer-songwriters. Acoustic and electronic music are a world apart, but it seems now that they can be closer than expected by the fact that both acoustic and electronic singer-songwriters exist. I define acoustic singer-songwriters as 1 person that sings and plays exclusively on 1 acoustic instrument, mainly guitar or piano. Examples are Passenger and Ed Sheeran. 

Electronic singer-songwriters are defined as people who produce their own music containing (not necessarily but also not limited to) synthesizers, looping stations or drums computers, and are able to perform it live. The music must contain roughly the same amount of singing as the acoustic songs. Examples are Chet Faker and JAIN. 

The question that will be examined in this report is: **What are the major differences between music of acoustic and electronic singer-songwriters and where do they overlap?**

In this investigation, the following analyses will be presented. When 2 songs are compared sections (2 to 5), the first song always is a song from the acoustic playlisy and the second song is one from the electronic playlist.

1. Mode, danceability, energy
2. Chromagram 'Dynabeat' and 'Will's lullaby'
3. SSM Timbre & Pitches 'North Swallow' and 'Clean'
4. Chordograms 'This' and 'LEKKER MET DE MEIDEN'
5. Keygrams 'Heart's On Fire' and 'Doesn't Matter (voleur de soleil)'

```{r}
acoustic <- get_playlist_audio_features('11122498650', '0a2aX4LlpTkeYLuTrVUu88')
electronic <- get_playlist_audio_features('11122498650', '2cANvNRpfzoZ6rkoaJQg25')

alles <- rbind(acoustic, electronic)

num_electronic <- nrow(electronic)
num_acoustic <- nrow(acoustic)
```

***

**Corpora**

There are two corpora that will be compared with each other. These are Spotify playlists. For the acoustic singer-songwriters, a playlist generated by Spotify that was called: **"Acoustic Singer-songwriters"** was chosen. The playlist was checked manually and all the songs satisfied the criteria stated above. This corpus represents the acoustic singer songwriters pretty well. There are **`r num_acoustic`** songs in this playlist.

<br/>

The playlist for the **electronic singer-songwriters** was made by a Spotify user and complemented by me.  It contains many songs that fulfill the criteria from above, but also some songs that belong to the indie genre, with lots of electric guitars, but no electronic instruments. There are **`r num_electronic`** songs in this playlist. These songs were manually filtered from this playlist as much as possible.

***

### Mode, energy, danceability

```{r}

energy_plot <- ggplot(alles, aes(x = danceability, y = energy, col = mode)) +
  geom_point() + 
  facet_wrap(~playlist_name) +
  labs(title = "Energy, danceability and mode", x = "Danceability", y = "Energy") 

ggplotly(energy_plot)
```

***

The biggest difference between the two playlists is the mode of the songs. In the acoustic playlist, **`r round((16 / num_acoustic * 100), digits = 2)`** percent of the songs is in minor key. 
In the electronic playlist, **`r round((74 / num_electronic * 100), digits = 2)`** percent of the songs is in minor. That is a large difference. That difference is displayed in the barplots shown in this graph. 

The difference in energy is notable. The electronic songs have a more energy than the acoustic songs, as you can see in the graph. 

The mean energy of the acoustic playlist is **`r round((acoustic %>% summarize(mean(energy))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(energy))), digits = 2)`)**. and the mean energy of the electronic playlist is ** `r round((electronic %>% summarize(mean(energy))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(energy))), digits = 2)`) **. 

The higher energy of the electronic playlist could be explained if the tempo of the electronic songs was higher than the tempo of the acoustic songs, but that is not the case, surprisingly. The mean tempo of the acoustic playlist is **`r round((acoustic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(tempo))), digits = 2)`)**. and the mean tempo of the electronic playlist is **`r round((electronic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(tempo))), digits = 2)`)**. 

The difference in danceability is also visible in the graph. There is not one song on the acoustic playlisy that has a danceability larger than 0.8. In the electronic playlist there are many songs that exceed that threshold. 

The mean danceability of the acoustic playlist is **`r round((acoustic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(danceability))), digits = 2)`)**. and the mean danceability of the electronic playlist is **`r round((electronic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(danceability))), digits = 2)`)**. 

From this section, we can conclude that electronic songs tend to be more moody than acoustic songs, due to the number of minor songs, acoustic songs are less danceable than electronic ones and have less energy.

**UPDATE**
Suddenly, summaries for the electronic playlist return 'NA', while the playlist hasn't changed and the code for computing the summaries haven't changed as well. I don't how to solve this problem, so I will ask dr. Bourgoyne the next lecture how to fix this.

### Chromograms 'Everything'll Be Alright' and 'Dynabeat'

![](graphs/chromagram_lullaby.png)  ![](graphs/chromagram_dynabeat.png)

```{r eval = F}
wills_lullaby_chroma <- 
    get_tidy_audio_analysis('5ULhLmoyMiCyd3a2pQZYsw') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
```

```{r eval = F}
wills_lullaby_chroma %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude', title = "'Everything'll Be Alright (Will's Lullaby') by Joshua Radin") +
    theme_minimal()
```

```{r eval = F}
dynabeat_chroma <- 
    get_tidy_audio_analysis('2n1Bg0FjuKy7YEKrGyheay') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
```


```{r eval = F}
dynabeat_chroma %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude', title = "'Dynabeat' by JAIN") +
    theme_minimal()
```

***

The chromagram of **'Everything'll be Alright'**  is overall darker than the chromagram of **'Dynabeat'**. The blue rectangles are wider than the other chromagram. That probably means those notes are hold longer than when the rectangles woudl be less wide. Longer notes could cause a lower energy. The energy of this song is 0.157 and the tempo is 170. The tempo measure seemed very high, so the tempo of this song was checked manually by tapping along on the website: http://www.beatsperminuteonline.com/. The outcome was 85 bpm. This is exactly half of the tempo Spotify measured and it makes sense, since one could tap twice as fast and then get 170 bpm as the tempo. Since most other acoustic songs has a tempo around 90 bpm, it seems that 85 bpm would be the correct tempo for **'Everything'll be Allright'**.

The **'Dynabeat'** chromagram has more blue than the previous one. That could be because more instruments are used at the same time causing more notes to be played at the same time. You could discriminate different sections in the song. It is also clear that the rectangles are very narrow, which could increase the energy and or tempo. The energy of this song is 0.613 and the tempo is 110. 


### Chroma- and Timbre Self Similarity Matrices ('North Swallow')

![](graphs/ssm_chroma_north.png) ![](graphs/ssm_timbre_north.png) 

```{r eval = F}
north <- 
    get_tidy_audio_analysis('3EapLR52jlyZQThOthMqpa') %>% 
    compmus_align(beats, segments) %>% 
    select(beats) %>% unnest(beats) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
      mutate(
          timbre = 
              map(segments, 
                  compmus_summarise, timbre, 
                  method = 'mean'))
```

```{r eval = F}
north %>% 
    compmus_self_similarity(pitches, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = "'North Swallow' by Barn Swallow (Chroma)")
```

```{r eval = F}
north %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = "'North Swallow' by Barn Swallow (Timbre)")
```



***

The chromabased SSM of **'North Swallow'** has a very clean checkerboard structure. The intro is used during the verses, which you can clearly see in the first checkerboard block. The bright yellow stripe is the first chorus, which is repeated 2 more times. The verses and chorusses are repeated with exactly the same sounds, which could be reason this SSM is so clean and tidy.

The timbre based SSM is not very bright because the song contains mainly acoustic guitar. There are barely any changes in the instruments that are used. There are drums, but they are roughly the same during the song. The timbre SSM of **'Clean'** is also less bright than the chroma SSM, but it is a bit brighter than this timbre SSM. 

### Chroma- and Timbre Self Similarity Matrices ('Clean')

![](graphs/ssm_chroma_clean.png) ![](graphs/ssm_timbre_clean.png) 

```{r eval = F}
clean <- 
    get_tidy_audio_analysis('2vIwwTufmg8YfZn5qvzxIg') %>% 
    compmus_align(beats, segments) %>% 
    select(beats) %>% unnest(beats) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
      mutate(
          timbre = 
              map(segments, 
                  compmus_summarise, timbre, 
                  method = 'mean'))
```

```{r eval = F}
clean %>% 
    compmus_self_similarity(pitches, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = "'Clean' by The Japanese House (Chroma)")
```

```{r eval = F}
clean %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = "'Clean' by The Japanese House (Timbre)")
```

***

The chroma- and timbrebased Self Similarity Matrices (SSM's) line up really well. The song has a chorus of 2 sentences and 2 verses. The chorusses are shown clearly in the three yellow lines at the beginning of the song. Later on, the lines appear again but they are surrounded by other yellow parts. This proabably is the part where 2 voices sing different parts at the same time, which causes the chorus to stand out less in this SSM. The chromabased SSM is overall very bright and yellow. It is brighter than the chromabased SSM of **'North Swallow'**. We've already seen this difference between the chromagrams of 'Everything'll be Alright' and **'Dynabeat'**. The differnce between the SSM's could as well explained by the fact that electronic musicians can use more instruments at the same time by looping them.

The chorusses are visible in the timbre SSM (the yellow vertical stripes), but the timbre of the chorusses are different. The instrumental music during the chorus changes almost everything it is sung, that is why the timbre is not exactly the same, but since it is the same part, you can still distinguish the chorusses in this SSM. 

We can conclude that the acoustic song, **'North Swallow'**,  has a clearer chroma structure than the electronic song, **'Clean'**, but that **'Clean'** has more timbre variation.


### Chordograms 'This' and 'LEKKER MET DE MEIDEN'

![](graphs/chordogram_bars_this.png) ![](graphs/chordogram_bars_meiden.png)

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3),
)

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
```


```{r eval = F}
this <- 
    get_tidy_audio_analysis('0pJfsPQesJyCnR5XWZyvj9') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r eval = F}
this %>% 
    compmus_match_pitch_template(chord_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'This' by Ed Sheeran")
```

```{r eval = F}
meiden <- 
    get_tidy_audio_analysis('299LhIMXI22rIBL96xjJ0S') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
```

```{r eval = F}
meiden %>% 
    compmus_match_pitch_template(chord_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'LEKKER MET DE MEIDEN' by MEROL")
```

***

**'This'** has roughly the same chords for the whole song, mostly B:maj/B:7 and E:maj/E:7. You can see this in the chordogram by the blue parts at the top. The song is pretty monotonous and doesn't change much.

**'LEKKER MET DE MEIDEN'** uses more different chords, but still the same chords over de whole song. Different sections are only distinguished by the yellow parts, but the chords that are played, are played the whole song. The chorus sound different than the verses, but it could be that the key of the chorus is the same as the verses and the same chords are just played differently.  

To investigate this further, I have also made keygrams of these to songs, show in the net tab.

### Keygrams 'This' and 'LEKKER MET DE MEIDEN'

![](graphs/keygram_this.png) ![](graphs/keygram_meiden.png) 

```{r eval = F}
key_this <- 
    get_tidy_audio_analysis('0pJfsPQesJyCnR5XWZyvj9') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
```

```{r eval = F}
key_this %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'This' by Ed Sheeran")
```

```{r eval = F}
key_meiden <- 
    get_tidy_audio_analysis('299LhIMXI22rIBL96xjJ0S') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r eval = F}
key_meiden %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'LEKKER MET DE MEIDEN' by MEROL")
```

***

In the keygram of **'This'**, one can see the different sections more clearly. The keygram is ofcourse aligned by sections, so the differences are clearer already, but you can now see two sections in the keygram around 100 and 150 ms that are different than the rest. These sections are indeed different than the rest when one listens to the song. At closer inspection of the chordogram, one can see these differences, but they're a lost less clear than in the keygram.

The keygram and chordogram of **'LEKKER MET DE MEIDEN'** is much more the same. The sections in the keygram are very visible in the chordogram as well. The key changes in the second section, which was not what was expected based on the chordogram. 

### Keygrams 'Heart's on Fire' and 'Doesn't Matter (voleur de soleil'

![](graphs/keygram_heartsonfire.png) ![](graphs/keygram_voleur.png) 

```{r eval = F}
heartsonfire <- 
    get_tidy_audio_analysis('07aKyFK2HxHj2Ikxo1gLoU') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
```

```{r eval = F}
heartsonfire %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'Heart's On Fire' by Passenger")
```

```{r eval = F}
voleur <- 
    get_tidy_audio_analysis('1E84UmsefPBdAZmUS4qrYv') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r eval = F}
voleur %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = "'Doesn't Matter (voleur de soleil' by Christine and the Queens")
```

***

These keygrams aren't all that different. **'Doesn't Matter'** has a large section where a lot of keys match the pitch information, whereas **'Heart's On Fire'** most of the time has one or two clear dark blue lines. This difference could again be explained by the use of multiple instruments at the same time in the electronic songs.

### Large Graph

```{r}
acoust <-
    get_playlist_audio_features('11122498650', 
                                '0a2aX4LlpTkeYLuTrVUu88') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
elect <-
    get_playlist_audio_features('11122498650', 
                                '2cANvNRpfzoZ6rkoaJQg25') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
singsong <-
    acoust %>% mutate(genre = "Acoustic") %>%
    bind_rows(elect %>% mutate(genre = "Electronic"))
```

```{r}
singsong %>% 
    mutate(
        sections = 
            map(
                sections, 
                summarise_at, 
                vars(tempo, loudness, duration), 
                list(section_mean = mean, section_sd = sd))) %>% 
    unnest(sections) %>%
    ggplot(
        aes(
            x = tempo, 
            y = tempo_section_sd, 
            colour = genre, 
            alpha = loudness)) +
    geom_point(aes(size = duration / 60)) + 
    geom_rug() + 
    theme_minimal() +
    ylim(0, 5) + 
    labs(
        x = 'Mean Tempo (bpm)', 
        y = 'SD Tempo', 
        colour = 'Genre', 
        size = 'Duration (min)', 
        alpha = 'Volume (dBFS)',
        title = "Title")
```


***

Since we're going to expand these kind of graphs next week, I will write down commentary and conclusions for this graph, that might next week.

### Timbre Coefficients Of the Playlists

```{r}
singsong %>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(genre, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = genre)) +
    geom_violin() +
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Genre', titel = "Timbre ofthe playlists")
```

***

The biggest timbre differences seem to be in coefficients c3, c4 and c5, but that should be verified with cepstrograms and listening tests of specific pieces, supported by the Spotify documentation for its timbre features.

### Conclusion

**Conclusion**

- Electronic more minor songs
- Electronic higher danceability and energy
- Electronic songs have more pitch classes in the songs
- Acoustic songs have a clearer structure in chordprogressions and keys.
- Electronic songs have more timbre variation

*** 

**Suggestions for improvement **

- Automatically compute percentage of minor songs.
- Write conclusion
- 

