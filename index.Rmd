---
title: "Acoustic vs. Electronic Singer-Songwriters"
author: "Jona Benja Bosman"
output: 
  flexdashboard::flex_dashboard:
        storyboard: true
---

```{r setup, message=F}
library(flexdashboard)
library(tidyverse)
library(plotly)
library(Cairo)
library(spotifyr)
library(ggplot2)
library(plyr)
library(dplyr)
library(compmus)
library(knitr)
source('spotify.R')
```

Acoustic vs. Electronic Singer-Songwriters
-----------------------------------------------------------------------
### Introduction {data-commentary-width=500}

**Introduction**

This assignment will be about the differences and similarities between electronic and acoustic singer-songwriters. Acoustic and electronic music are a world apart, but it seems now that they can be closer than expected by the fact that both acoustic and electronic singer-songwriters exist. I define acoustic singer-songwriters as 1 or max. 2 people that sing and play exclusively on 1 acoustic instrument, mainly guitar or piano. Examples are Passenger and Ed Sheeran. Electronic singer-songwriters are defined as people who produce their own music containing (not necessarily but also not limited to) synthesizers, looping stations or drums computers, and are able to perform it live. The music must contain roughly the same amount of singing as the acoustic songs. Examples are Chet Faker and JAIN. 

The question that will be examined in this report is: **Apart from the instruments used, what are the major differences between music of acoustic and electronic singer-songwriters?**

```{r}
acoustic <- get_playlist_audio_features('11122498650', '0a2aX4LlpTkeYLuTrVUu88')
electronic <- get_playlist_audio_features('11122498650', '2cANvNRpfzoZ6rkoaJQg25')

alles <- rbind(acoustic, electronic)

num_electronic <- nrow(electronic)
num_acoustic <- nrow(acoustic)
```

***

**Corpora**

There are two corpora that will be compared with each other. These are Spotify playlists. For the acoustic singer-songwriters, a playlist generated by Spotify that was called: **"Acoustic Singer-songwriters"** was chosen. The playlist was checked manually and all the songs satisfied the criteria stated above. This corpus represents the acoustic singer songwriters pretty well. There are **`r num_acoustic`** songs in this playlist.

<br/>

The playlist for the **electronic singer-songwriters** was made by a Spotify user and complemented by me.  It contains many songs that fulfill the criteria from above, but also more songs that belong to the Indie genre, with lots of electric guitars, but no electronic instruments. There are **`r num_electronic`** songs in this playlist.

***

### Mode, energy, danceability

```{r}

energy_plot <- ggplot(alles, aes(x = danceability, y = energy, col = mode)) +
  geom_point() + 
  facet_wrap(~playlist_name) +
  labs(title = "Energy, danceability and mode", x = "Danceability", y = "Energy") 

ggplotly(energy_plot)

```

***

The biggest difference between the two playlists is the mode of the songs. In the acoustic playlist, `r round((33 / num_acoustic * 100), digits = 2) ` percent of the songs is in minor key. 
In the electronic playlist, `r round((83 / num_electronic * 100), digits = 2) ` percent of the songs is in minor. That is a large difference. That difference is displayed in the barplots shown in this graph. 

The difference in energy is notable. The electronic songs have a more energy than the acoustic songs, as you can see in the graph. 

The mean energy of the acoustic playlist is `r round((acoustic %>% summarize(mean(energy))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(energy))), digits = 2)`). and the mean energy of the electronic playlist is `r round((electronic %>% summarize(mean(energy))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(energy))), digits = 2)`). 


The higher energy of the electronic playlist could be explained if the tempo of the electronic songs was higher than the tempo of the acoustic songs, but that is not the case, surprisingly. The mean tempo of the acoustic playlist is `r round((acoustic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(tempo))), digits = 2)`). and the mean tempo of the electronic playlist is `r round((electronic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(tempo))), digits = 2)`). 

The difference in danceability is also visible in the graph. There is not one song on the acoustic playlisy that has a danceability larger than 0.8. In the electronic playlist there are many song that exceed that threshold. 

The mean danceability of the acoustic playlist is `r round((acoustic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(danceability))), digits = 2)`). and the mean danceability of the electronic playlist is `r round((electronic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(danceability))), digits = 2)`). 

Maybe acoustic songs are more danceable than we think, or it could be that Spotify included 'slowdancing' in the 'danceable' criterium.


### Chromogram Everything'll Be Alright (Wills Lullaby)

![](chroma_lullaby.png)  ![](chroma_dynabeat.png)

```{r eval = F}
wills_lullaby_chroma <- 
    get_tidy_audio_analysis('5ULhLmoyMiCyd3a2pQZYsw') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
```

```{r eval = F}
wills_lullaby_chroma %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_minimal()
```

***

This chromagram is overall very dark. It doesn't have many sections or a lot of pitch classes. The blue rectangles are wider than the chromagram on the other page. Maybe that would mean that those notes are hold longer than if the rectangles were less wide. Longer notes could cause a lower energy. The energy of this song is 0.157 and the tempo is 170. I think the tempo measure is wrong. 

This chromagram is much bluer than the previous one. That could be because more instruments are used at the same time causing more notes to be played at the same time. You could discriminate different sections in the song. It is also clear that the rectangles are very narrow, which could increase the energy and or tempo. The energy of this song is 0.613 and the tempo is 110. 


```{r eval = F}
dynabeat_chroma <- 
    get_tidy_audio_analysis('2n1Bg0FjuKy7YEKrGyheay') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
```


```{r eval = F}
dynabeat_chroma %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_minimal()
```


### Cepstogram & SSM'Clean' by The Japanese House

![](cepstrogram_clean.png)  ![](ssm_clean.png)

```{r eval = F}
clean <- 
    get_tidy_audio_analysis('2vIwwTufmg8YfZn5qvzxIg') %>% 
    compmus_align(beats, segments) %>% 
    select(beats) %>% unnest(beats) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
      mutate(
          timbre = 
              map(segments, 
                  compmus_summarise, timbre, 
                  method = 'mean'))
```

```{r eval = F}
clean %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    scale_fill_viridis_c(option = 'E') +
    theme_classic()
```

***

Comments

### SSM 'Clean' by The Japanese House

```{r eval = F}
clean %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')
```

***

it works now!

### Week 10

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3),
)

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
```


```{r}
twenty_five <- 
    get_tidy_audio_analysis('5UVsbUV0Kh033cqsZ5sLQi') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))
    
```

```{r}
twenty_five %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')
```

***

### Week 10.2

```{r}
bebop <-
    get_playlist_audio_features(
        'thesoundsofspotify', 
        '55s8gstHcaCyfU47mQgLrB') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
bigband <-
    get_playlist_audio_features(
        'thesoundsofspotify', 
        '2cjIvuw4VVOQSeUAZfNiqY') %>% 
    slice(1:30) %>% 
    add_audio_analysis()
jazz <-
    bebop %>% mutate(genre = "Bebop") %>%
    bind_rows(bigband %>% mutate(genre = "Big Band"))
```

```{r}
jazz %>% 
    mutate(
        sections = 
            map(
                sections, 
                summarise_at, 
                vars(tempo, loudness, duration), 
                list(section_mean = mean, section_sd = sd))) %>% 
    unnest(sections) %>%
    ggplot(
        aes(
            x = tempo, 
            y = tempo_section_sd, 
            colour = genre, 
            alpha = loudness)) +
    geom_point(aes(size = duration / 60)) + 
    geom_rug() + 
    theme_minimal() +
    ylim(0, 5) + 
    labs(
        x = 'Mean Tempo (bpm)', 
        y = 'SD Tempo', 
        colour = 'Genre', 
        size = 'Duration (min)', 
        alpha = 'Volume (dBFS)')
```

***

### Week 10.3

```{r}
jazz %>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(genre, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = genre)) +
    geom_violin() +
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Genre')
```

***


### Conclusion

**Conclusion**

The playlists are more alike than different, but there are some interesting features to explore, mainly the differences in mode and energy. The chance that an electronic song is minor are higher than the chance an acoustic song is minor. Electronic songs also tend to have more energy than acoustic songs, and a higher danceability.

The chromagram of an electronic song has many more different sections and more different pitch classes than acoustic songs. If this differnce would be generalizable, one could say that electronic singer-songwriter songs are more variably than acoustic singer-songwriter songs.

*** 

**Suggestions for improvement **
* Automatically compute percentage of minor songs.
* Get SSMs to display (on the same page)
* Get both chromagrams on the same page.

