---
title: "Acoustic vs. Electronic Singer-Songwriters"
author: "Jona Benja Bosman"
output: 
  flexdashboard::flex_dashboard:
        storyboard: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r message=F}
library(flexdashboard)
library(tidyverse)
library(tidymodels)
library(recipes)
library(plotly)
library(Cairo)
library(spotifyr)
library(ggplot2)
library(plyr)
library(dplyr)
library(compmus)
library(knitr)
library(robustbase)
library(ggdendro)
library(protoclust)
library(heatmaply)
source('spotify.R')
```

### Introduction {data-commentary-width=500}

**Introduction**

In this storyboard, I wil try to convince you of the existance of the genre Electronic Singer-Songwriters.  Acoustic and electronic music are a world apart. The music is very different which is inherently caused by the use of different instruments.  

When asked 'What is a singer-songwriter?' many people will think about a person who justs playes guitar and sings. This would be an acoustic singer-songwriter.

I define acoustic singer-songwriters as one person that sings and plays exclusively on one acoustic instrument, mainly guitar or piano. Examples are Passenger and Ed Sheeran. In most acoustic singer-songwriter songs, a little of percussion is included as well, for example a cajon.

I argue that electronic singer-songwriters exist as well. 

Electronic singer-songwriters are defined as people who produce their own music containing (not necessarily but also not limited to) synthesizers, looping stations or drums computers, and are able to perform it live. The music must contain roughly the same amount of singing as the acoustic songs. Examples are Chet Faker and JAIN. 

The question that will be examined in this report is: **Does there exists a genre Electronic Singer-Songwriters that is different from Acoustic Song-Songwriters?**

This implies that there have to be noticeble differences between the two genres, but also similarities that represent singer-songwriters.

In this investigation, 2 corpora consisting of 227 songs per genre will be compared. For more information about the playlists, see the 'Copera' section. 

First, the two corpora will be compared as a whole by a classifcation and clustering of all the songs. Then the track-level features like danceability and acousticness will be compared among the corpora. Per corpus, 2 representative songs will be chosen and will be subject to a chromagram, self-similarity matrix, keygram and tempogram analyses.

Lastly, the 2 corpora will be compared on Spotify timbre coefficients.


```{r}
acoustic <- get_playlist_audio_features('11122498650', '0a2aX4LlpTkeYLuTrVUu88')
electronic <- get_playlist_audio_features('11122498650', '2cANvNRpfzoZ6rkoaJQg25')

alles <- rbind(acoustic, electronic)

num_electronic <- nrow(electronic)
num_acoustic <- nrow(acoustic)
```

***

**Corpora**

There are two corpora that will be compared with each other. These are Spotify playlists. For the acoustic singer-songwriters, a playlist generated by Spotify that was called: **"Acoustic Singer-songwriters"** was chosen. The playlist was checked manually and all the songs satisfied the criteria stated above. This corpus represents the acoustic singer songwriters pretty well. There are **`r num_acoustic`** songs in this playlist.

<br/>

The playlist for the **electronic singer-songwriters** was made by a Spotify user and complemented by me. There are **`r num_electronic`** songs in this playlist. It contains many songs that fulfill the criteria from above, but also some songs that belong to the indie genre, with lots of electric guitars, but no electronic instruments. These songs were manually filtered from this playlist as much as possible.

### Confusion matrix

![](graphs/conf_mat.png)

***
*Acoustic Singer-Songwriters:*

Accuracy: **`r round ((186 / (186 + 47) * 100), digits = 2)`**

Precision: **`r round ((186 / (186 + 68) * 100), digits = 2)`** 

<br/>

*Electronic Singer-Songwriters:*

Accuracy: **`r round ((159 / (159 + 68) * 100), digits = 2)`** 

Precision: **`r round ((159 / (159 + 47) * 100), digits = 2)`**

The results of the confusion matrix are promising. Most of the songs are classified in the right genre. The accuracy is not incredibly high, but enough evidence that the genres are different.

### Distinguishing features. 
![](graphs/random_forests.png)

***

The most distinguishing features between the two playlists are: 

* Pitch class C
* Timbre features c04, c06 and c08
* Danceability
* Speechiness
* Acousticness 

The mean danceability of the acoustic playlist is **`r round((acoustic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(danceability))), digits = 2)`)**. and the mean danceability of the electronic playlist is **`r round((electronic %>% summarize(mean(danceability))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(danceability))), digits = 2)`)**. 

The mean speechiness of the acoustic playlist is **`r round((acoustic %>% summarize(mean(speechiness))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(speechiness))), digits = 2)`)**. and the mean speechiness of the electronic playlist is **`r round((electronic %>% summarize(mean(speechiness))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(speechiness))), digits = 2)`)**. 

The mean acousticness of the acoustic playlist is **`r round((acoustic %>% summarize(mean(acousticness))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(acousticness))), digits = 2)`)**. and the mean acousticness of the electronic playlist is **`r round((electronic %>% summarize(mean(acousticness))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(acousticness))), digits = 2)`)**. 

To find a representative songs per playlists, songs that have featuresvalues around these means will be chosen.

Acoustic playlist: 'Disappears With Time' by Tangerine (acousticess = 0.664).

Electronic playlist: 'Maybe You're The Reason' by The Japanese House (danceability = 0.621).

**Timbre features**
Timbre feature c04 describes songs with a stronger attack.
Timbre feature c06 describes songs that start with a low energy, persumably an intro, have a bright middle, and a medium end. 
Timbre feature c08 describes songs that start with bright high frequencies and end with bright low frequencies.

The timbre coefficients will be discussed more thouroughly in the last section of this notebook. 


### Dendrogram both playlists
![](graphs/dendrogram_fifty_songs.png)


***

To see whether the playlists would be seperated in a clustering task, a new playlist was created with 25 representative acoustic songs and 25 representative electronic songs. The acoustic songs were selected so that their acousticness was around the mean acousticness of the whole acoustic playlist. The electronic songs were selected in the same way, but danceability was used instead of acousticness.

Ideally, the dendrogram would show us two large clusters with one containing all 25 acoustic songs and one containing all 25 electronic songs. This is not the case, but the songs are clustered in groups that consists almost solely of electronic or acoustic songs. The song 'Why Don't You Listen' is the only song that occurs on it's own.

This clustering shown by the dendrogram could mean that the songs clearly beling to a different genre and are grouped together with this genre based on their features. 

Along withe results of the confusion matrix, this is evidence for the existance of the genre 'Electronic Singer-Songwriters'. 

### Track-level features (differences)

```{r}
track_plot <- ggplot(alles, aes(x = danceability, y = acousticness, col = mode, alpha = instrumentalness)) +
  geom_jitter() +
  facet_wrap(~playlist_name) +
  labs(title = "Acousticness, danceability, mode and energy", x = "Danceability", y = "Acousticness") 

ggplotly(track_plot)
```

```{r}
acoustic_minor <- acoustic %>%
      filter (mode == "minor")

num_acoustic_minor <- nrow(acoustic_minor)

electronic_minor <- electronic %>%
      filter (mode == "minor")
  
num_electronic_minor <- nrow(electronic_minor)
```

***
**Mode**

This graph shows the noticable differences between the playlists. The largest difference seems to be mode. There are `r num_acoustic_minor` minor songs in the acoustic playlist, which is a percentage of **`r round ((num_acoustic_minor / num_acoustic * 100), digits = 2)`%**. 

In the electronic playlist, there are `r num_electronic_minor` songs, which is a percentage of **`r round ((num_electronic_minor / num_electronic * 100), digits = 2) `%**.

**Acousticness**
The acoustic songs are more acoustic than the electronic songs, which makes a lot of sense. 

**Danceability**
None of the acoustic songs has a danceability higher than 0.8. Of the electronic songsa lot more have a danceability higher than or close to 0.8. There seems to be a small trade-off between acousticness and danceability. 

**Instrumentalness**
And lastly, electronic songs seem to be more instrumental than acoustic songs.

The mean instrumentalness of the acoustic playlist is **`r round((acoustic %>% summarize(mean(instrumentalness))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(instrumentalness))), digits = 2)`)**. and the mean instrumentalness of the electronic playlist is **`r round((electronic %>% summarize(mean(instrumentalness))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(instrumentalness))), digits = 2)`)**. 

### Track-level features (similarities)

```{r}
track_plot <- ggplot(alles, aes(x = speechiness, y = tempo, alpha = loudness)) +
  geom_jitter() + 
  facet_wrap(~playlist_name) +
  labs(title = "Tempo, speechiness and loudness ", x = "Tempo", y = "Speechiness") 

ggplotly(track_plot)
```

*** 
These graphs seems very similar, except for the tempo. The acoustic songs all have roughly the same tempo, but the electronic songs have more variety in tempo and overall a higher tempo. 

The mean tempo of the acoustic playlist is **`r round((acoustic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((acoustic %>% summarize(sd(tempo))), digits = 2)`)**. and the mean tempo of the electronic playlist is **`r round((electronic %>% summarize(mean(tempo))), digits = 2)`, (SD = `r round  ((electronic %>% summarize(sd(tempo))), digits = 2)`)**. 

This difference is very small, so it belongs in this graph.

### Chromagrams 'Disappears With Time' and 'Maybe You're The Reason'
![](graphs/disappears_chroma.png) ![](graphs/reason_chroma.png)

*** 

Two differences can be seen between these chromagrams:


1. In 'Maybe You're The Reason', more pitch classes are used than in 'Disappears In Time'. This could be explained by the fact that more different instruments can be used by electronic singer-songwriters due to the use of a machine that can play multiple sounds at the same time. 

2. In 'Disappears With Time', the blue rectangles are wider than in 'Maybe You're The Reason'. This could mean that the notes or chords in 'Disappears With Time' are held longer, which could be a clue that the tempo of that song is lower. The tempo of 'Disappears With Time is 127 BPM and the tempo of 'Maybe You're The Reason' is 95 BPM so this is not true. Another explanation could be that there are more chords in 'Disappears With Time' and that 'Maybe You're The Reason' has more notes played in succesion instead of together. When these songs are listend to, this seems to be the case. 

Recall that one of the distinctive differences between the playlists found was pitch class C. 'Maybe You're The Reason' has more energy on pitch class C than 'Disappears With Time'. This could very well be accidental, but it is worth showing. It could be that electronic songs are relatively simpler with regards to use of keys and chords, since the variation in sounds can be bigger. This could result in more songs being in the key of C, which is the most 'basic' key. Acoustic muscians are maybe pushed to write more variation in key since they can only use one instrument and their voice. 

To investigate this more, a graph displaying the used keys per playlist is shown on the next tab.

This one example could be a little bit of evidence that electronic songs use more successive notes and acoustic songs use more chords.

### Used keys

```{r}
keys <- ggplot(alles, stat = count, aes(x = key, fill = key)) +
  geom_bar() + 
  facet_wrap(~playlist_name)

ggplotly(keys)
```

***

```{r}
c_electronic <- nrow(electronic %>% filter(key == 'C'))
c_acoustic <- nrow(acoustic %>% filter(key == 'C'))
```

C is the most used key in the electronic playlist, but it is also the most used key in the acoustic playlist. It is true that C is used more in electronic songs than in acoustic songs, but the difference is not big. 

`r c_acoustic` of the in total `r num_acoustic` acoustic songs are in C, which is a percentage of **`r round((c_acoustic / num_acoustic * 100), digits = 0)`%**.

`r c_electronic` of the in total `r num_electronic` electronic songs are in C, which is a percentage of **`r round((c_electronic / num_electronic * 100), digits = 0)`%**.


This difference seems too small too draw conclusions from.

### SSM's Chroma 'Disappears With Time' and 'Maybe You're The Reason'
![](graphs/disappears_ssm_chroma.png) ![](graphs/reason_ssm_chroma.png)

***

When looking at the chroma Self Similarity Matrices, it seems that the 'Disappears With Time' is more structured in the sense that different sections alternate regularly. The SSM shows a checkerboard. In the song, there is a riff that is repeated very often and the rest of the song is very repetitive. 

'Maybe You're The Reason' is less structured and this is visible in the chromagram as well. Especially in the verses, not a lot of music is played, there is just singing. There is an intro riff that is repeated in the chorus, but the chorus contains a lot of other instruments. The verses and chorusses are not always the same. This explains the chroma-based SSM. 

Basd on this example, it could carefully be posed that acoustic songs seems t be more repetitive and structured, but to be able to strengthen this hypothesis, the timbre-based SSM's have to be compared, which is done on the next page. 

### SSM's Timbre 'Disappears With Time' and 'Maybe You're The Reason'
![](graphs/disappears_ssm_timbre.png) ![](graphs/reason_ssm_timbre.png)

***
The timbre-based SSM of 'Disappears With Time' is less bright than the chromagram, but equally structured. The song has roughly the same timbre during the whole time; there is not much variation in sound. 

'Maybe You're The Reason' has a more structured timbre- than chroma-based SSM. The timbre of the verses and chorusses is clearly different and separable. So 'Maybe You're The Reason' has more structured variation.

This difference can partly be explained by the possibility for electronic songs to use more instruments, since it is easier to make variation with more instruments. Nevertheless,  our hypthesis does still stand: 

Acoustic songs seem to have more repetition in their chroma's and timbre and electronic songs seem to have more variation.

### Tempograms 
![](graphs/disappears_tempogram.png) ![](graphs/reason_tempogram.png)

***
The difference in tempograms is very clear: 'Maybe You're The Reason' keeps a very steady tempo and 'Disappears With Time' shows a shakier line. This is easily explained: 'Maybe You're The Reason' is probably produced with a computer beat that maintains the exact same tempo the whole song and 'Disappears With Time' is probably recordeed while someone was playing the guitar live.

In most but not all cases the drums of electronic songs are produced with a drum computer and therefore have a very steady tempo.

It can be concluded that the tempo of electronic songs, provided that the drums were created with a drum computer have a steadier tempo than acoustic songs.

### Timbre coefficients

```{r}
acoust <-
    get_playlist_audio_features('11122498650', 
                                '0a2aX4LlpTkeYLuTrVUu88') %>% 
    add_audio_analysis()
elect <-
    get_playlist_audio_features('11122498650', 
                                '2cANvNRpfzoZ6rkoaJQg25') %>% 
    add_audio_analysis()
singsong <-
    acoust %>% mutate(genre = "Acoustic") %>%
    bind_rows(elect %>% mutate(genre = "Electronic"))
```

```{r}
timbre_coefficients <-
  singsong %>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(genre, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = genre)) +
    geom_violin() +
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Genre', titel = "Timbre ofthe playlists")

ggplotly(timbre_coefficients)
```

*** 

To return on the timbre coefficients: 

The timbre coefficients that were the most different between the playlists, according to the randomForests, were coefficients c04, c06, and c08. 

This timbre coefficients graph is in accordance with this. The biggest differences are in these 3 coefficients. This graph shows us how these differences are expressed. 

The electronic playlist has more of timbre features c04 and c08. 
This would mean that electronic songs tend to have a stronger attack and start with instruments that have higher frequencies and end with songs that have lower frequencies, like bass.

The acoustic playlist has more of timbre feature c06. 
This would mean that acoustic songs tend to start low-energy, presumably a soft intro, build up in the middle part and end with less energy, presumably an outro. 

### Sum up & Conclusion
We have compared the two corpora in a lot of ways and found similarities and differences. 

The classifier that was build could group around 70% songs in the right genre, but 30% of the songs were grouped wrong. This means that there is similarity between the corpora up to a certain degree, but that they are mainly different. The dendrogram shows the same results.

The main similarities that were found were the speechiness, tempo, loudness and most of the Spotify timbre coefficients.

Electronic songs tend to be more moody (more minor songs) than acoustic ones, have more instrumental breaks and are more danceable. They also have more variation in pitch classes and a steadier tempo. With regards to the timbre, electronic songs have a stronger attack.

Acoustic songs tend to have more structure, both in their pitch classes and timbre. They also tend to have more soft intro's and outro's. 

Based on these conclusions, we can say with some confidence that Electronic Singer-Songwriters make music in a genre that is different from but comparable with the music that acoustic singer-songwriters make. 

It should be noted that a large part of the comparison was performed on 2 songs that were representative for each of the corpora, but to draw a stronger conclusion, more songs have to be compared.

***
**Link to the playlists:**

*Acoustic playlist*: https://open.spotify.com/user/11122498650/playlist/0a2aX4LlpTkeYLuTrVUu88?si=Nrjnwh-gRqSFDZyBX9zjMg 

*Electronic playlist*: https://open.spotify.com/user/11122498650/playlist/2cANvNRpfzoZ6rkoaJQg25?si=0Uive1_3RFuKjGakTXaKyQ
